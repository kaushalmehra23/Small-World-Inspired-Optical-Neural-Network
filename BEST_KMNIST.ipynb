{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ONN MODEL WITH 10 LAYERS WITH SMALL WORLD SKIP CONNECTION ON KMNIST WITH SA"
      ],
      "metadata": {
        "id": "KT7t7W4J26o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import KMNIST\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "\n",
        "# --- Config ---\n",
        "detector_layout = [3, 4, 3]  # three rows: 3,4,3 detectors = 10 classes\n",
        "wavelength = 480e-9\n",
        "image_size = 14\n",
        "tiles = 6\n",
        "output_size = tiles * image_size  # 84\n",
        "min_skip_distance = 2\n",
        "max_skip_distance = 7\n",
        "\n",
        "# Saturable absorber base parameters\n",
        "base_A0 = 0.1\n",
        "base_I_sat = 5e6\n",
        "base_A_ns = 0.005\n",
        "\n",
        "# Training parameters\n",
        "epochs = 15\n",
        "patience = 5\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Dataset (MNIST) ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "full_train = KMNIST(root='./data', train=True, download=True, transform=transform)  # KMNIST dataset\n",
        "test_dataset = KMNIST(root='./data', train=False, download=True, transform=transform)  # KMNIST dataset\n",
        "\n",
        "# Split train/val\n",
        "total = len(full_train)\n",
        "val_size = int(0.1 * total)\n",
        "train_size = total - val_size\n",
        "train_set, val_set = random_split(full_train, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# --- Model ---\n",
        "class OpticalLayer(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.phase = nn.Parameter(torch.randn(size, size) * 0.1)\n",
        "    def forward(self, x, kernel):\n",
        "        x = x * torch.exp(1j * self.phase)\n",
        "        return torch.fft.ifft2(torch.fft.fft2(x) * kernel)\n",
        "\n",
        "class FastONNRandomSkipSA(nn.Module):\n",
        "    def __init__(self, num_layers=10, cascade_sas=2):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList([OpticalLayer(output_size) for _ in range(num_layers)])\n",
        "        self.detector_scale = nn.Parameter(torch.tensor(10.0))\n",
        "        self.fft_grid = self._create_fft_grid().to(device)\n",
        "        full_masks = self._create_full_detector_masks().to(device)\n",
        "        self.detector_masks = full_masks[:num_classes]  # shape [10, H, W]\n",
        "\n",
        "        # Gains and SA params\n",
        "        self.gains = nn.ParameterList([nn.Parameter(torch.tensor(1.0)) for _ in range(num_layers)])\n",
        "        self.A0 = nn.ParameterList([nn.Parameter(torch.tensor(base_A0)) for _ in range(num_layers)])\n",
        "        self.I_sat = nn.ParameterList([nn.Parameter(torch.tensor(base_I_sat)) for _ in range(num_layers)])\n",
        "        self.A_ns = nn.ParameterList([nn.Parameter(torch.tensor(base_A_ns)) for _ in range(num_layers)])\n",
        "        self.L = nn.ParameterList([nn.Parameter(torch.tensor(1.0)) for _ in range(num_layers)])\n",
        "        self.cascade_sas = cascade_sas\n",
        "\n",
        "        # Skip connections\n",
        "        self.skip_connections = self._generate_random_connections()\n",
        "        self.skip_weights = nn.ParameterDict()\n",
        "        self.skip_phases = nn.ParameterDict()\n",
        "        for s, t in self.skip_connections:\n",
        "            key = f\"{s}_{t}\"\n",
        "            self.skip_weights[key] = nn.Parameter(torch.tensor(0.5))\n",
        "            self.skip_phases[key] = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def _generate_random_connections(self):\n",
        "        conns = []\n",
        "        for s in range(self.num_layers - min_skip_distance):\n",
        "            for t in range(s + min_skip_distance, min(s + max_skip_distance + 1, self.num_layers)):\n",
        "                if random.random() < rewiring_prob:\n",
        "                    conns.append((s, t))\n",
        "        print(f\"Generated {len(conns)} random skip connections (p={rewiring_prob})\")\n",
        "        return conns\n",
        "\n",
        "    def _create_fft_grid(self):\n",
        "        fx = torch.fft.fftfreq(output_size, d=1e-6)\n",
        "        fy = torch.fft.fftfreq(output_size, d=1e-6)\n",
        "        FX, FY = torch.meshgrid(fx, fy, indexing='xy')\n",
        "        k = 2 * np.pi / wavelength\n",
        "        arg = torch.clamp(1 - (wavelength * FX)**2 - (wavelength * FY)**2, 0.0)\n",
        "        return torch.exp(1j * k * torch.sqrt(arg))\n",
        "\n",
        "    def _create_full_detector_masks(self):\n",
        "        masks = []\n",
        "        H = W = output_size\n",
        "        rows = len(detector_layout)\n",
        "        band_h = H // rows\n",
        "        for r, cols in enumerate(detector_layout):\n",
        "            cell_w = W // cols\n",
        "            y0 = r * band_h; y1 = y0 + band_h\n",
        "            for c in range(cols):\n",
        "                x0 = c * cell_w; x1 = x0 + cell_w\n",
        "                m = torch.zeros(H, W)\n",
        "                m[y0:y1, x0:x1] = 1.0\n",
        "                masks.append(m)\n",
        "        return torch.stack(masks)\n",
        "\n",
        "    def tile_input(self, x):\n",
        "        B = x.size(0)\n",
        "        x = x.view(B, 1, 1, image_size, image_size)\n",
        "        x = x.repeat(1, tiles, tiles, 1, 1)\n",
        "        x = x.permute(0, 1, 3, 2, 4).reshape(B, output_size, output_size)\n",
        "        return x.to(torch.complex64)\n",
        "\n",
        "    def _apply_gain(self, field, idx):\n",
        "        return field * self.gains[idx]\n",
        "\n",
        "    def _apply_sa(self, field, idx):\n",
        "        out = field\n",
        "        for _ in range(self.cascade_sas):\n",
        "            I = out.real**2 + out.imag**2\n",
        "            T = torch.exp(-self.L[idx] * ((self.A0[idx] / (1 + I / self.I_sat[idx])) + self.A_ns[idx]))\n",
        "            out = torch.sqrt(T) * out\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tile_input(x)\n",
        "        inter = {}\n",
        "        for i in range(self.num_layers):\n",
        "            # skip merges\n",
        "            for s, t in self.skip_connections:\n",
        "                if t == i and s in inter:\n",
        "                    key = f\"{s}_{t}\"\n",
        "                    w = torch.sigmoid(self.skip_weights[key])\n",
        "                    p = torch.exp(1j * self.skip_phases[key])\n",
        "                    x = x + w * (inter[s] * p)\n",
        "            # optical + gain + SA\n",
        "            x = self.layers[i](x, self.fft_grid)\n",
        "            x = self._apply_gain(x, i)\n",
        "            x = self._apply_sa(x, i)\n",
        "            inter[i] = x.clone()\n",
        "        # detection\n",
        "        I = x.real**2 + x.imag**2\n",
        "        raw = (I.unsqueeze(1) * self.detector_masks.unsqueeze(0)).sum(dim=(2,3))\n",
        "        return raw * self.detector_scale\n",
        "\n",
        "# --- Train/Eval ---\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    corr = total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            out = model(imgs)\n",
        "            corr += (out.argmax(1) == lbls).sum().item()\n",
        "            total += lbls.size(0)\n",
        "    return 100.0 * corr / total\n",
        "\n",
        "for p in [0.0,0.1,0.2,0.3,0.5,0.7,1.0]:\n",
        "    rewiring_prob = p\n",
        "    random.seed(42); torch.manual_seed(42)\n",
        "    print(f\"\\n=== p={p} ===\")\n",
        "    model = FastONNRandomSkipSA(num_layers=10, cascade_sas=2).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "    best_val = 0; patience_cnt = 0\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train(); corr = total = 0; start = time.time()\n",
        "        for imgs, lbls in train_loader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(imgs)\n",
        "            loss = nn.CrossEntropyLoss()(out, lbls)\n",
        "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "            optimizer.step()\n",
        "            corr += (out.argmax(1)==lbls).sum().item(); total += lbls.size(0)\n",
        "        scheduler.step(); val_acc = evaluate(model,val_loader)\n",
        "        print(f\"Epoch {epoch}/{epochs} | Train {100*corr/total:.2f}% | Val {val_acc:.2f}% | Time {time.time()-start:.1f}s\")\n",
        "        if val_acc>best_val: best_val, patience_cnt = val_acc, 0\n",
        "        else: patience_cnt+=1\n",
        "        if patience_cnt>=patience: print('Early stopping'); break\n",
        "    test_acc = evaluate(model,test_loader)\n",
        "    print(f\"Best Val {best_val:.2f}% | Test {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCtphhAYnzj4",
        "outputId": "d6919c2d-37e7-40ff-f024-fb3f7cccc260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18.2M/18.2M [00:27<00:00, 662kB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 132kB/s]\n",
            "100%|██████████| 3.04M/3.04M [00:05<00:00, 586kB/s]\n",
            "100%|██████████| 5.12k/5.12k [00:00<00:00, 6.27MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== p=0.0 ===\n",
            "Generated 0 random skip connections (p=0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | Train 87.52% | Val 91.28% | Time 27.3s\n",
            "Epoch 2/15 | Train 93.18% | Val 93.90% | Time 27.5s\n",
            "Epoch 3/15 | Train 94.29% | Val 93.82% | Time 28.4s\n",
            "Epoch 4/15 | Train 94.85% | Val 93.43% | Time 27.5s\n",
            "Epoch 5/15 | Train 94.90% | Val 94.33% | Time 27.4s\n",
            "Epoch 6/15 | Train 96.68% | Val 95.43% | Time 27.4s\n",
            "Epoch 7/15 | Train 96.92% | Val 95.17% | Time 27.5s\n",
            "Epoch 8/15 | Train 97.16% | Val 95.70% | Time 28.0s\n",
            "Epoch 9/15 | Train 97.33% | Val 95.53% | Time 27.4s\n",
            "Epoch 10/15 | Train 97.30% | Val 95.55% | Time 27.4s\n",
            "Epoch 11/15 | Train 98.34% | Val 95.77% | Time 27.5s\n",
            "Epoch 12/15 | Train 98.50% | Val 95.88% | Time 27.3s\n",
            "Epoch 13/15 | Train 98.63% | Val 95.65% | Time 28.0s\n",
            "Epoch 14/15 | Train 98.71% | Val 95.82% | Time 27.4s\n",
            "Epoch 15/15 | Train 98.81% | Val 95.67% | Time 27.3s\n",
            "Best Val 95.88% | Test 89.87%\n",
            "\n",
            "=== p=0.1 ===\n",
            "Generated 7 random skip connections (p=0.1)\n",
            "Epoch 1/15 | Train 89.06% | Val 93.80% | Time 30.7s\n",
            "Epoch 2/15 | Train 94.80% | Val 94.28% | Time 30.1s\n",
            "Epoch 3/15 | Train 95.85% | Val 95.05% | Time 30.2s\n",
            "Epoch 4/15 | Train 96.26% | Val 95.37% | Time 30.5s\n",
            "Epoch 5/15 | Train 96.67% | Val 94.78% | Time 30.2s\n",
            "Epoch 6/15 | Train 97.98% | Val 95.50% | Time 30.5s\n",
            "Epoch 7/15 | Train 98.36% | Val 95.65% | Time 30.1s\n",
            "Epoch 8/15 | Train 98.41% | Val 95.82% | Time 30.6s\n",
            "Epoch 9/15 | Train 98.56% | Val 95.57% | Time 30.1s\n",
            "Epoch 10/15 | Train 98.57% | Val 95.73% | Time 30.2s\n",
            "Epoch 11/15 | Train 99.29% | Val 95.97% | Time 30.4s\n",
            "Epoch 12/15 | Train 99.46% | Val 95.88% | Time 30.1s\n",
            "Epoch 13/15 | Train 99.49% | Val 95.98% | Time 30.4s\n",
            "Epoch 14/15 | Train 99.48% | Val 95.55% | Time 30.2s\n",
            "Epoch 15/15 | Train 99.44% | Val 95.85% | Time 30.6s\n",
            "Best Val 95.98% | Test 90.05%\n",
            "\n",
            "=== p=0.2 ===\n",
            "Generated 9 random skip connections (p=0.2)\n",
            "Epoch 1/15 | Train 89.50% | Val 93.45% | Time 30.9s\n",
            "Epoch 2/15 | Train 94.94% | Val 94.63% | Time 31.2s\n",
            "Epoch 3/15 | Train 95.82% | Val 95.10% | Time 30.8s\n",
            "Epoch 4/15 | Train 96.36% | Val 95.18% | Time 31.2s\n",
            "Epoch 5/15 | Train 96.65% | Val 95.07% | Time 31.0s\n",
            "Epoch 6/15 | Train 97.98% | Val 95.73% | Time 31.2s\n",
            "Epoch 7/15 | Train 98.35% | Val 95.53% | Time 30.7s\n",
            "Epoch 8/15 | Train 98.43% | Val 95.73% | Time 31.0s\n",
            "Epoch 9/15 | Train 98.48% | Val 95.88% | Time 30.9s\n",
            "Epoch 10/15 | Train 98.68% | Val 95.95% | Time 31.2s\n",
            "Epoch 11/15 | Train 99.28% | Val 95.98% | Time 30.9s\n",
            "Epoch 12/15 | Train 99.42% | Val 95.90% | Time 31.2s\n",
            "Epoch 13/15 | Train 99.49% | Val 95.92% | Time 31.0s\n",
            "Epoch 14/15 | Train 99.46% | Val 96.07% | Time 31.7s\n",
            "Epoch 15/15 | Train 99.47% | Val 96.15% | Time 31.1s\n",
            "Best Val 96.15% | Test 90.33%\n",
            "\n",
            "=== p=0.3 ===\n",
            "Generated 13 random skip connections (p=0.3)\n",
            "Epoch 1/15 | Train 88.48% | Val 93.13% | Time 32.8s\n",
            "Epoch 2/15 | Train 94.91% | Val 94.60% | Time 33.0s\n",
            "Epoch 3/15 | Train 95.87% | Val 95.00% | Time 32.4s\n",
            "Epoch 4/15 | Train 96.53% | Val 95.35% | Time 32.7s\n",
            "Epoch 5/15 | Train 96.93% | Val 94.68% | Time 32.5s\n",
            "Epoch 6/15 | Train 98.12% | Val 95.75% | Time 32.8s\n",
            "Epoch 7/15 | Train 98.42% | Val 95.50% | Time 32.9s\n",
            "Epoch 8/15 | Train 98.51% | Val 95.72% | Time 32.5s\n",
            "Epoch 9/15 | Train 98.66% | Val 95.90% | Time 32.8s\n",
            "Epoch 10/15 | Train 98.70% | Val 96.02% | Time 33.0s\n",
            "Epoch 11/15 | Train 99.32% | Val 96.00% | Time 32.4s\n",
            "Epoch 12/15 | Train 99.56% | Val 96.18% | Time 32.9s\n",
            "Epoch 13/15 | Train 99.53% | Val 96.02% | Time 32.8s\n",
            "Epoch 14/15 | Train 99.60% | Val 95.80% | Time 32.6s\n",
            "Epoch 15/15 | Train 99.51% | Val 95.97% | Time 32.7s\n",
            "Best Val 96.18% | Test 89.79%\n",
            "\n",
            "=== p=0.5 ===\n",
            "Generated 16 random skip connections (p=0.5)\n",
            "Epoch 1/15 | Train 88.65% | Val 93.75% | Time 33.7s\n",
            "Epoch 2/15 | Train 95.03% | Val 94.87% | Time 33.3s\n",
            "Epoch 3/15 | Train 96.06% | Val 94.95% | Time 33.8s\n",
            "Epoch 4/15 | Train 96.64% | Val 95.12% | Time 33.7s\n",
            "Epoch 5/15 | Train 96.96% | Val 94.93% | Time 34.1s\n",
            "Epoch 6/15 | Train 98.22% | Val 95.95% | Time 33.5s\n",
            "Epoch 7/15 | Train 98.53% | Val 95.78% | Time 33.8s\n",
            "Epoch 8/15 | Train 98.63% | Val 95.92% | Time 33.6s\n",
            "Epoch 9/15 | Train 98.64% | Val 95.93% | Time 33.4s\n",
            "Epoch 10/15 | Train 98.79% | Val 95.48% | Time 33.7s\n",
            "Epoch 11/15 | Train 99.32% | Val 95.93% | Time 33.6s\n",
            "Early stopping\n",
            "Best Val 95.95% | Test 90.48%\n",
            "\n",
            "=== p=0.7 ===\n",
            "Generated 25 random skip connections (p=0.7)\n",
            "Epoch 1/15 | Train 87.33% | Val 93.38% | Time 37.1s\n",
            "Epoch 2/15 | Train 94.71% | Val 94.10% | Time 37.2s\n",
            "Epoch 3/15 | Train 95.81% | Val 95.05% | Time 37.2s\n",
            "Epoch 4/15 | Train 96.61% | Val 94.97% | Time 37.2s\n",
            "Epoch 5/15 | Train 96.99% | Val 95.08% | Time 37.1s\n",
            "Epoch 6/15 | Train 98.16% | Val 95.45% | Time 37.2s\n",
            "Epoch 7/15 | Train 98.53% | Val 95.53% | Time 37.2s\n",
            "Epoch 8/15 | Train 98.64% | Val 95.75% | Time 37.3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtdGmniZ2-4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}