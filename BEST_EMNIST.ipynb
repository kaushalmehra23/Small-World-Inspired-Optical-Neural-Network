{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ONN MODEL WITH 10 LAYERS, WITH SMALL WORLD CONNECTIONS AND SATURABLE ABSORBER. DATASET EMNIST\n"
      ],
      "metadata": {
        "id": "rpYPVJyPFHem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "\n",
        "# --- Config ---\n",
        "detector_layout = [8, 8, 8, 8, 8, 7]  # six rows: five of 8, one of 7 = 47 classes for EMNIST Balanced\n",
        "wavelength = 480e-9\n",
        "image_size = 14\n",
        "tiles = 6\n",
        "output_size = tiles * image_size  # 84\n",
        "min_skip_distance = 2  # Minimum layers to skip\n",
        "max_skip_distance = 7  # Maximum skip distance\n",
        "# rewiring_prob will be set in the sweep\n",
        "\n",
        "# Saturable absorber base parameters (modifiable)\n",
        "base_A0 = 0.1         # small-signal absorption\n",
        "base_I_sat = 5e6      # saturation intensity (|E|^2 units)\n",
        "base_A_ns = 0.005     # non-saturable loss\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Dataset (EMNIST Balanced) ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "full_train = EMNIST(root=\"./data\", split=\"balanced\", train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root=\"./data\", split=\"balanced\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Create train / validation split\n",
        "total_train = len(full_train)\n",
        "val_size = int(0.1 * total_train)\n",
        "train_size = total_train - val_size\n",
        "train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --- Compute number of classes ---\n",
        "num_classes = sum(detector_layout)\n",
        "assert num_classes == 47, f\"Sum of detector_layout must be 47, got {num_classes}\"\n",
        "\n",
        "# --- Model definitions ---\n",
        "class OpticalLayer(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.phase = nn.Parameter(torch.randn(size, size) * 0.1)\n",
        "\n",
        "    def forward(self, x, kernel):\n",
        "        x = x * torch.exp(1j * self.phase)\n",
        "        x_fft = torch.fft.fft2(x)\n",
        "        x_fft = x_fft * kernel\n",
        "        return torch.fft.ifft2(x_fft)\n",
        "\n",
        "class FastONNRandomSkipSA(nn.Module):\n",
        "    def __init__(self, num_layers=10, cascade_sas=2):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList([OpticalLayer(output_size) for _ in range(num_layers)])\n",
        "        self.detector_scale = nn.Parameter(torch.tensor([10.0], dtype=torch.float32))\n",
        "        self.fft_grid = self._create_fft_grid().to(device)\n",
        "        self.detector_masks = self._create_detector_masks().to(device)\n",
        "\n",
        "        # Purely optical gain stages per layer\n",
        "        self.gains = nn.ParameterList([nn.Parameter(torch.tensor(1.0)) for _ in range(num_layers)])\n",
        "\n",
        "        # Saturable absorber parameters (learnable or tunable per layer)\n",
        "        self.A0 = nn.ParameterList([nn.Parameter(torch.tensor(base_A0)) for _ in range(num_layers)])\n",
        "        self.I_sat = nn.ParameterList([nn.Parameter(torch.tensor(base_I_sat)) for _ in range(num_layers)])\n",
        "        self.A_ns = nn.ParameterList([nn.Parameter(torch.tensor(base_A_ns)) for _ in range(num_layers)])\n",
        "        # Tunable absorber length factor per layer\n",
        "        self.L = nn.ParameterList([nn.Parameter(torch.tensor(1.0)) for _ in range(num_layers)])\n",
        "        # Number of cascaded SA elements\n",
        "        self.cascade_sas = cascade_sas\n",
        "\n",
        "        # Skip connections\n",
        "        self.skip_connections = self._generate_random_connections()\n",
        "        self.skip_weights = nn.ParameterDict()\n",
        "        self.skip_phases = nn.ParameterDict()\n",
        "        for src, tgt in self.skip_connections:\n",
        "            key = f\"{src}_{tgt}\"\n",
        "            self.skip_weights[key] = nn.Parameter(torch.tensor(0.5))\n",
        "            self.skip_phases[key] = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def _generate_random_connections(self):\n",
        "        connections = []\n",
        "        for src in range(self.num_layers - min_skip_distance):\n",
        "            max_tgt = min(src + max_skip_distance, self.num_layers - 1)\n",
        "            for tgt in range(src + min_skip_distance, max_tgt + 1):\n",
        "                if random.random() < rewiring_prob:\n",
        "                    connections.append((src, tgt))\n",
        "        print(f\"Generated {len(connections)} random skip connections (p={rewiring_prob})\")\n",
        "        return connections\n",
        "\n",
        "    def _create_fft_grid(self):\n",
        "        fx = torch.fft.fftfreq(output_size, d=1e-6)\n",
        "        fy = torch.fft.fftfreq(output_size, d=1e-6)\n",
        "        FX, FY = torch.meshgrid(fx, fy, indexing='xy')\n",
        "        k = 2 * np.pi / wavelength\n",
        "        arg = 1 - (wavelength * FX)**2 - (wavelength * FY)**2\n",
        "        arg = torch.clamp(arg, min=0.0)\n",
        "        return torch.exp(1j * k * torch.sqrt(arg))\n",
        "\n",
        "    def _create_detector_masks(self):\n",
        "        masks = []\n",
        "        H = W = output_size\n",
        "        rows = len(detector_layout)\n",
        "        band_h = H // rows\n",
        "        for r, cols in enumerate(detector_layout):\n",
        "            cell_w = W // cols\n",
        "            det_h, det_w = band_h, cell_w\n",
        "            y_center = r * band_h + band_h // 2\n",
        "            y0 = max(0, y_center - det_h // 2)\n",
        "            y1 = min(H, y0 + det_h)\n",
        "            for c in range(cols):\n",
        "                x_center = c * cell_w + cell_w // 2\n",
        "                x0 = max(0, x_center - det_w // 2)\n",
        "                x1 = min(W, x0 + det_w)\n",
        "                m = torch.zeros(H, W)\n",
        "                m[y0:y1, x0:x1] = 1.0\n",
        "                masks.append(m)\n",
        "        masks = torch.stack(masks)\n",
        "        assert masks.shape[0] == num_classes, f\"Expected {num_classes} masks, got {masks.shape[0]}\"\n",
        "        return masks\n",
        "\n",
        "    def tile_input(self, x):\n",
        "        B = x.size(0)\n",
        "        x = x.view(B, 1, 1, image_size, image_size)\n",
        "        x = x.repeat(1, tiles, tiles, 1, 1)\n",
        "        x = x.permute(0, 1, 3, 2, 4).reshape(B, output_size, output_size)\n",
        "        return x.to(torch.complex64)\n",
        "\n",
        "    def _apply_gain(self, field, layer_idx):\n",
        "        return field * self.gains[layer_idx]\n",
        "\n",
        "    def _apply_sa(self, field, layer_idx):\n",
        "        # cascade multiple SA elements for steeper nonlinearity\n",
        "        out = field\n",
        "        for _ in range(self.cascade_sas):\n",
        "            intensity = out.real**2 + out.imag**2\n",
        "            T = torch.exp(-self.L[layer_idx] * ((self.A0[layer_idx] / (1 + intensity / self.I_sat[layer_idx])) + self.A_ns[layer_idx]))\n",
        "            out = torch.sqrt(T) * out\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tile_input(x)\n",
        "        intermediate = {}\n",
        "        for idx in range(self.num_layers):\n",
        "            # merge skip paths\n",
        "            for src, tgt in self.skip_connections:\n",
        "                if tgt == idx and src in intermediate:\n",
        "                    key = f\"{src}_{tgt}\"\n",
        "                    w = torch.sigmoid(self.skip_weights[key])\n",
        "                    p = torch.exp(1j * self.skip_phases[key])\n",
        "                    x = x + w * (intermediate[src] * p)\n",
        "\n",
        "            # optical layer\n",
        "            x = self.layers[idx](x, self.fft_grid)\n",
        "            # gain stage\n",
        "            x = self._apply_gain(x, idx)\n",
        "            # saturable absorber nonlinearity\n",
        "            x = self._apply_sa(x, idx)\n",
        "            # store for skips\n",
        "            intermediate[idx] = x.clone()\n",
        "\n",
        "        # detection\n",
        "        intensity = x.real**2 + x.imag**2\n",
        "        raw = (intensity.unsqueeze(1) * self.detector_masks.unsqueeze(0)).sum(dim=(2,3))\n",
        "        return raw * self.detector_scale\n",
        "\n",
        "# --- Evaluation & Training ---\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs, lbls = imgs.squeeze(1).to(device), lbls.to(device)\n",
        "            out = model(imgs)\n",
        "            correct += (out.argmax(1) == lbls).sum().item()\n",
        "            total += lbls.size(0)\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, test_loader, epochs=15, lr=0.01):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "    best_val = 0.0\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        loss_sum, corr, tot = 0.0, 0, 0\n",
        "        start = time.time()\n",
        "        for imgs, lbls in train_loader:\n",
        "            imgs, lbls = imgs.squeeze(1).to(device), lbls.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(imgs)\n",
        "            loss = criterion(out, lbls)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()*lbls.size(0)\n",
        "            corr += (out.argmax(1)==lbls).sum().item()\n",
        "            tot += lbls.size(0)\n",
        "        scheduler.step()\n",
        "        train_acc = 100*corr/tot\n",
        "        val_acc = evaluate(model, val_loader)\n",
        "        print(f\"Epoch {epoch}/{epochs} | Loss: {loss_sum/tot:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Time: {time.time()-start:.1f}s\")\n",
        "        best_val = max(best_val, val_acc)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f\"Best Val Acc: {best_val:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
        "    return test_acc\n",
        "\n",
        "# --- Hyperparameter Sweep ---\n",
        "for p in [0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]:\n",
        "    rewiring_prob = p\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    print(f\"\\n=== p={p} ===\")\n",
        "    model = FastONNRandomSkipSA(num_layers=10, cascade_sas=2).to(device)\n",
        "    _ = train_model(model, train_loader, val_loader, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOwiFwz_FEqz",
        "outputId": "b0680817-c766-4cfa-8ffc-3048c508ddd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:02<00:00, 200MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== p=0.0 ===\n",
            "Generated 0 random skip connections (p=0.0)\n",
            "Epoch 1/15 | Loss: 1.2531 | Train Acc: 66.98% | Val Acc: 72.05% | Time: 54.5s\n",
            "Epoch 2/15 | Loss: 0.9309 | Train Acc: 74.69% | Val Acc: 73.90% | Time: 54.4s\n",
            "Epoch 3/15 | Loss: 0.8566 | Train Acc: 76.28% | Val Acc: 76.61% | Time: 52.9s\n",
            "Epoch 4/15 | Loss: 0.8036 | Train Acc: 77.56% | Val Acc: 76.40% | Time: 53.0s\n",
            "Epoch 5/15 | Loss: 0.7787 | Train Acc: 77.95% | Val Acc: 75.62% | Time: 53.3s\n",
            "Epoch 6/15 | Loss: 0.6375 | Train Acc: 81.24% | Val Acc: 79.88% | Time: 53.3s\n",
            "Epoch 7/15 | Loss: 0.6022 | Train Acc: 81.97% | Val Acc: 80.13% | Time: 53.0s\n",
            "Epoch 8/15 | Loss: 0.5910 | Train Acc: 82.43% | Val Acc: 80.45% | Time: 52.9s\n",
            "Epoch 9/15 | Loss: 0.5758 | Train Acc: 82.59% | Val Acc: 79.87% | Time: 53.6s\n",
            "Epoch 10/15 | Loss: 0.5590 | Train Acc: 82.90% | Val Acc: 80.92% | Time: 53.0s\n",
            "Epoch 11/15 | Loss: 0.4814 | Train Acc: 84.94% | Val Acc: 81.86% | Time: 53.0s\n",
            "Epoch 12/15 | Loss: 0.4619 | Train Acc: 85.34% | Val Acc: 82.49% | Time: 53.2s\n",
            "Epoch 13/15 | Loss: 0.4540 | Train Acc: 85.59% | Val Acc: 82.03% | Time: 53.2s\n",
            "Epoch 14/15 | Loss: 0.4483 | Train Acc: 85.74% | Val Acc: 81.75% | Time: 53.2s\n",
            "Epoch 15/15 | Loss: 0.4364 | Train Acc: 86.05% | Val Acc: 81.75% | Time: 53.0s\n",
            "Best Val Acc: 82.49% | Test Acc: 81.64%\n",
            "\n",
            "=== p=0.1 ===\n",
            "Generated 7 random skip connections (p=0.1)\n",
            "Epoch 1/15 | Loss: 1.1653 | Train Acc: 69.93% | Val Acc: 74.92% | Time: 58.2s\n",
            "Epoch 2/15 | Loss: 0.7906 | Train Acc: 77.68% | Val Acc: 77.20% | Time: 58.3s\n",
            "Epoch 3/15 | Loss: 0.7166 | Train Acc: 79.24% | Val Acc: 78.87% | Time: 58.4s\n",
            "Epoch 4/15 | Loss: 0.6790 | Train Acc: 80.25% | Val Acc: 78.71% | Time: 58.0s\n",
            "Epoch 5/15 | Loss: 0.6505 | Train Acc: 80.73% | Val Acc: 79.73% | Time: 58.2s\n",
            "Epoch 6/15 | Loss: 0.5328 | Train Acc: 83.64% | Val Acc: 80.71% | Time: 58.3s\n",
            "Epoch 7/15 | Loss: 0.5087 | Train Acc: 84.19% | Val Acc: 81.59% | Time: 58.9s\n",
            "Epoch 8/15 | Loss: 0.4939 | Train Acc: 84.63% | Val Acc: 80.73% | Time: 57.9s\n",
            "Epoch 9/15 | Loss: 0.4805 | Train Acc: 84.83% | Val Acc: 80.97% | Time: 58.1s\n",
            "Epoch 10/15 | Loss: 0.4724 | Train Acc: 85.11% | Val Acc: 81.56% | Time: 58.4s\n",
            "Epoch 11/15 | Loss: 0.4022 | Train Acc: 86.96% | Val Acc: 82.92% | Time: 57.9s\n",
            "Epoch 12/15 | Loss: 0.3846 | Train Acc: 87.36% | Val Acc: 82.84% | Time: 58.4s\n",
            "Epoch 13/15 | Loss: 0.3783 | Train Acc: 87.56% | Val Acc: 82.66% | Time: 58.3s\n",
            "Epoch 14/15 | Loss: 0.3720 | Train Acc: 87.73% | Val Acc: 82.00% | Time: 58.2s\n",
            "Epoch 15/15 | Loss: 0.3643 | Train Acc: 88.00% | Val Acc: 82.36% | Time: 58.2s\n",
            "Best Val Acc: 82.92% | Test Acc: 82.62%\n",
            "\n",
            "=== p=0.2 ===\n",
            "Generated 9 random skip connections (p=0.2)\n",
            "Epoch 1/15 | Loss: 1.2230 | Train Acc: 69.90% | Val Acc: 75.23% | Time: 59.5s\n",
            "Epoch 2/15 | Loss: 0.7916 | Train Acc: 77.75% | Val Acc: 77.30% | Time: 59.8s\n",
            "Epoch 3/15 | Loss: 0.7198 | Train Acc: 79.25% | Val Acc: 77.86% | Time: 59.8s\n",
            "Epoch 4/15 | Loss: 0.6786 | Train Acc: 80.33% | Val Acc: 78.83% | Time: 59.7s\n",
            "Epoch 5/15 | Loss: 0.6540 | Train Acc: 80.68% | Val Acc: 79.27% | Time: 60.1s\n",
            "Epoch 6/15 | Loss: 0.5319 | Train Acc: 83.63% | Val Acc: 80.60% | Time: 59.4s\n",
            "Epoch 7/15 | Loss: 0.5070 | Train Acc: 84.32% | Val Acc: 81.29% | Time: 59.8s\n",
            "Epoch 8/15 | Loss: 0.4932 | Train Acc: 84.61% | Val Acc: 80.94% | Time: 59.9s\n",
            "Epoch 9/15 | Loss: 0.4787 | Train Acc: 85.00% | Val Acc: 80.73% | Time: 59.8s\n",
            "Epoch 10/15 | Loss: 0.4704 | Train Acc: 85.14% | Val Acc: 81.81% | Time: 59.9s\n",
            "Epoch 11/15 | Loss: 0.3996 | Train Acc: 87.00% | Val Acc: 82.46% | Time: 60.2s\n",
            "Epoch 12/15 | Loss: 0.3819 | Train Acc: 87.39% | Val Acc: 82.80% | Time: 59.4s\n",
            "Epoch 13/15 | Loss: 0.3757 | Train Acc: 87.64% | Val Acc: 82.53% | Time: 59.9s\n",
            "Epoch 14/15 | Loss: 0.3699 | Train Acc: 87.82% | Val Acc: 81.93% | Time: 59.7s\n",
            "Epoch 15/15 | Loss: 0.3613 | Train Acc: 88.04% | Val Acc: 82.77% | Time: 59.6s\n",
            "Best Val Acc: 82.80% | Test Acc: 82.78%\n",
            "\n",
            "=== p=0.3 ===\n",
            "Generated 13 random skip connections (p=0.3)\n",
            "Epoch 1/15 | Loss: 1.2904 | Train Acc: 69.56% | Val Acc: 75.54% | Time: 62.4s\n",
            "Epoch 2/15 | Loss: 0.7881 | Train Acc: 77.83% | Val Acc: 77.81% | Time: 62.5s\n",
            "Epoch 3/15 | Loss: 0.7098 | Train Acc: 79.55% | Val Acc: 78.76% | Time: 62.7s\n",
            "Epoch 4/15 | Loss: 0.6635 | Train Acc: 80.60% | Val Acc: 78.67% | Time: 62.4s\n",
            "Epoch 5/15 | Loss: 0.6378 | Train Acc: 81.09% | Val Acc: 79.43% | Time: 62.4s\n",
            "Epoch 6/15 | Loss: 0.5216 | Train Acc: 83.86% | Val Acc: 80.66% | Time: 62.7s\n",
            "Epoch 7/15 | Loss: 0.4970 | Train Acc: 84.63% | Val Acc: 81.46% | Time: 62.9s\n",
            "Epoch 8/15 | Loss: 0.4839 | Train Acc: 84.87% | Val Acc: 80.94% | Time: 63.1s\n",
            "Epoch 9/15 | Loss: 0.4685 | Train Acc: 85.22% | Val Acc: 80.92% | Time: 62.7s\n",
            "Epoch 10/15 | Loss: 0.4606 | Train Acc: 85.37% | Val Acc: 81.89% | Time: 62.6s\n",
            "Epoch 11/15 | Loss: 0.3914 | Train Acc: 87.15% | Val Acc: 82.53% | Time: 62.4s\n",
            "Epoch 12/15 | Loss: 0.3742 | Train Acc: 87.57% | Val Acc: 82.67% | Time: 62.4s\n",
            "Epoch 13/15 | Loss: 0.3686 | Train Acc: 87.76% | Val Acc: 82.30% | Time: 62.6s\n",
            "Epoch 14/15 | Loss: 0.3623 | Train Acc: 87.95% | Val Acc: 81.76% | Time: 62.5s\n",
            "Epoch 15/15 | Loss: 0.3545 | Train Acc: 88.18% | Val Acc: 82.42% | Time: 62.5s\n",
            "Best Val Acc: 82.67% | Test Acc: 82.49%\n",
            "\n",
            "=== p=0.5 ===\n",
            "Generated 16 random skip connections (p=0.5)\n",
            "Epoch 1/15 | Loss: 1.4496 | Train Acc: 69.60% | Val Acc: 75.88% | Time: 65.6s\n",
            "Epoch 2/15 | Loss: 0.7717 | Train Acc: 78.09% | Val Acc: 78.21% | Time: 65.0s\n",
            "Epoch 3/15 | Loss: 0.6999 | Train Acc: 79.84% | Val Acc: 79.00% | Time: 65.0s\n",
            "Epoch 4/15 | Loss: 0.6564 | Train Acc: 80.77% | Val Acc: 78.95% | Time: 64.8s\n",
            "Epoch 5/15 | Loss: 0.6314 | Train Acc: 81.31% | Val Acc: 79.66% | Time: 65.7s\n",
            "Epoch 6/15 | Loss: 0.5170 | Train Acc: 83.99% | Val Acc: 80.54% | Time: 64.9s\n",
            "Epoch 7/15 | Loss: 0.4937 | Train Acc: 84.58% | Val Acc: 81.80% | Time: 64.9s\n",
            "Epoch 8/15 | Loss: 0.4791 | Train Acc: 85.05% | Val Acc: 81.26% | Time: 64.9s\n",
            "Epoch 9/15 | Loss: 0.4660 | Train Acc: 85.23% | Val Acc: 81.18% | Time: 65.7s\n",
            "Epoch 10/15 | Loss: 0.4570 | Train Acc: 85.38% | Val Acc: 82.23% | Time: 65.1s\n",
            "Epoch 11/15 | Loss: 0.3900 | Train Acc: 87.27% | Val Acc: 82.45% | Time: 64.7s\n",
            "Epoch 12/15 | Loss: 0.3740 | Train Acc: 87.62% | Val Acc: 82.66% | Time: 65.1s\n",
            "Epoch 13/15 | Loss: 0.3670 | Train Acc: 87.88% | Val Acc: 82.62% | Time: 65.7s\n",
            "Epoch 14/15 | Loss: 0.3623 | Train Acc: 87.97% | Val Acc: 82.26% | Time: 65.0s\n",
            "Epoch 15/15 | Loss: 0.3541 | Train Acc: 88.20% | Val Acc: 82.68% | Time: 65.1s\n",
            "Best Val Acc: 82.68% | Test Acc: 82.46%\n",
            "\n",
            "=== p=0.7 ===\n",
            "Generated 25 random skip connections (p=0.7)\n",
            "Epoch 1/15 | Loss: 3.6083 | Train Acc: 68.27% | Val Acc: 75.27% | Time: 71.6s\n",
            "Epoch 2/15 | Loss: 0.7853 | Train Acc: 77.93% | Val Acc: 78.02% | Time: 71.7s\n",
            "Epoch 3/15 | Loss: 0.7003 | Train Acc: 79.69% | Val Acc: 78.88% | Time: 71.6s\n",
            "Epoch 4/15 | Loss: 0.6499 | Train Acc: 80.87% | Val Acc: 79.24% | Time: 72.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtdGmniZ2-4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}